{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1a8e9138-841f-4bfd-ac5b-2e72579ed6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "41ff0c6a-45ba-40ab-801a-0e1be9750266",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DIR = \"audios\"   \n",
    "SAMPLE_RATE = 44100    \n",
    "DURATION = 3           \n",
    "N_MELS = 128           \n",
    "TEST_SIZE = 0.2        \n",
    "USE_MFCC = False       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fd1c08b3-c1cd-4486-93ce-561553f91ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path, sample_rate, duration, n_mels, use_mfcc=False):\n",
    "    y, sr = librosa.load(path, sr=sample_rate)\n",
    "    target_len = int(sample_rate * duration)\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "    \n",
    "    if use_mfcc:\n",
    "        features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mels)\n",
    "    else:\n",
    "        features = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
    "        features = librosa.power_to_db(features, ref=np.max)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6b4b0522-4bd5-439e-8517-e623fc3dc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(audio_dir):\n",
    "    X, y = [], []\n",
    "    labels = [\"non_wake\", \"wake\"]\n",
    "    for label_idx, label in enumerate(labels):\n",
    "        folder = os.path.join(audio_dir, label)\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith(\".wav\"):\n",
    "                path = os.path.join(folder, file)\n",
    "                feats = extract_features(path, SAMPLE_RATE, DURATION, N_MELS, USE_MFCC)\n",
    "                X.append(feats)\n",
    "                y.append(label_idx)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    X = X[..., np.newaxis]            # (N, H, W, 1)\n",
    "    X = np.transpose(X, (0, 3, 1, 2)) # (N, 1, H, W)\n",
    "    return X, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c3c8e061-caab-49d6-9fd6-db5f16be17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, labels = load_dataset(AUDIO_DIR)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, stratify=y)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4b3debc7-cabb-4027-b46e-a8440f28707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WakeWordCNN(nn.Module):\n",
    "    def __init__(self, input_channels: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self._to_linear = None\n",
    "        self.classifier = None\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x.view(x.size(0), -1).shape[1]\n",
    "            self.classifier = nn.Linear(self._to_linear, self.output_shape).to(x.device)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "90f2ca24-dc2c-4670-ab4f-e5c280505a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c061e913-6c1d-4777-83b2-e494944071cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = WakeWordCNN(input_channels=1, hidden_units=32, output_shape=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ca8693a7-8432-489b-83c1-e50a0ce812f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = (y_true == y_pred).sum().item()\n",
    "    acc = correct / len(y_true) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bed3d67f-45a7-441b-a7b2-202e68ebe831",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e9370a8e-d9e5-48a8-9222-36c1ff34539e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8386aef7b0ee47d6b006baa7d7426af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training time on mps: 1.42 sec\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_time_start = timer()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    #print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_loss = 0\n",
    "    model_0.train()\n",
    "\n",
    "    for X_batch, y_batch in train_dataloader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model_0(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "   \n",
    "    test_loss, test_acc = 0, 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X_batch, y_batch in test_dataloader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            test_pred = model_0(X_batch)\n",
    "            test_loss += loss_fn(test_pred, y_batch).item()\n",
    "            test_acc += accuracy_fn(y_batch, test_pred.argmax(dim=1))\n",
    "\n",
    "    test_loss /= len(test_dataloader)\n",
    "    test_acc /= len(test_dataloader)\n",
    "\n",
    "    #print(f\"Train loss: {train_loss:.4f} | Test loss: {test_loss:.4f} | Test acc: {test_acc:.2f}%\")\n",
    "\n",
    "train_time_end = timer()\n",
    "print(f\"\\n Training time on {device}: {train_time_end - train_time_start:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "15df5239-9c26-47b7-aee3-ba13940a3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, accuracy_fn, device = device):\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            y_pred = model(X)\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y, y_pred.argmax(dim=1))\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__,\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2ddc662b-e941-4188-aa1c-802140ec39b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'WakeWordCNN',\n",
       " 'model_loss': 0.6929569840431213,\n",
       " 'model_acc': 57.14285714285714}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0_results = eval_model(model_0, test_dataloader, loss_fn, accuracy_fn, device)\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "26e80cc8-0704-416d-b292-de4c51ad8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "Record completed.\n",
      "Prediction : wake (probability 64.14%)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "def record_audio(filename, duration, samplerate=44100):\n",
    "    print(\"Recording...\")\n",
    "    recording = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1, dtype='int16')\n",
    "    sd.wait()\n",
    "    print(\"Record completed.\")\n",
    "    wav.write(filename, samplerate, recording)\n",
    "\n",
    "duration = 3\n",
    "filename = \"test_audio.wav\"\n",
    "record_audio(filename, duration)\n",
    "model_0.eval()\n",
    "\n",
    "# Charger l'audio à prédire\n",
    "audio_path = \"test_audio.wav\"\n",
    "features = extract_features(audio_path, SAMPLE_RATE, DURATION, N_MELS, USE_MFCC)\n",
    "\n",
    "# Formater comme batch pour le CNN\n",
    "X = np.expand_dims(features, axis=(0,1))  # (1, 1, H, W)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_0.to(device)\n",
    "\n",
    "\n",
    "X = X.to(device)   # <-- Très important !\n",
    "\n",
    "\n",
    "with torch.inference_mode():\n",
    "    preds = model_0(X)\n",
    "    probs = F.softmax(preds, dim=1)\n",
    "    pred_class = torch.argmax(probs, dim=1).item()\n",
    "    confidence = probs[0][pred_class].item()\n",
    "\n",
    "\n",
    "labels = [\"non_wake\", \"wake\"]\n",
    "\n",
    "print(f\"Prediction : {labels[pred_class]} (probability {confidence*100:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
